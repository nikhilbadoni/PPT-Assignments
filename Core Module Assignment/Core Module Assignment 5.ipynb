{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68aa4cff",
   "metadata": {},
   "source": [
    "# Core Module | Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f48db91",
   "metadata": {},
   "source": [
    "## Naive Bayes Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a74e8e7",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "### What is the Naive Approach in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b8260",
   "metadata": {},
   "source": [
    "The Naive Approach in machine learning refers to a simple and straightforward method that assumes all features in a dataset are independent of each other. This assumption is called naive because it often doesn't hold true in real-world scenarios. The most common example is the Naive Bayes classifier, where it assumes that the presence of one particular feature does not affect the presence of another. Despite its simplifying assumption, the Naive Approach can be surprisingly effective, especially when dealing with large datasets, text classification, and certain types of data distributions. However, its performance may suffer when faced with highly correlated features or complex relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e3ed59",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "### Explain the assumptions of feature independence in the Naive Approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f078b",
   "metadata": {},
   "source": [
    "The Naive Approach makes a fundamental assumption known as the \"feature independence assumption.\" This assumption states that the presence or absence of one feature is independent of the presence or absence of any other feature in the dataset. \n",
    "\n",
    "- Conditional Independence: Given the class label, all features are assumed to be conditionally independent of each other. This means that knowing the value of one feature provides no information about the values of other features.\n",
    "- Single Feature Importance: Each feature contributes independently and equally to the classification decision. There are no interactions or dependencies between features that influence the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e62e3",
   "metadata": {},
   "source": [
    "# Q3.\n",
    "### How does the Naive Approach handle missing values in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3896d",
   "metadata": {},
   "source": [
    "The Naive Approach handles missing values in a straightforward manner:\n",
    "\n",
    "- During Training: When calculating probabilities for each feature given a class label, the Naive Bayes classifier ignores instances with missing values for that particular feature. This means that missing values do not contribute to the probability estimation of the feature.\n",
    "- During Prediction: When making predictions for new data with missing values, the Naive Bayes classifier skips the feature(s) with missing values in the probability calculation. The prediction is still made based on the available features.\n",
    "\n",
    "It's important to note that handling missing values by ignoring instances or features can lead to biased or inaccurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf099604",
   "metadata": {},
   "source": [
    "# Q4.\n",
    "### What are the advantages and disadvantages of the Naive Approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6983063",
   "metadata": {},
   "source": [
    "Advantages of the Naive Approach:\n",
    "\n",
    "- Simplicity: The Naive Approach is easy to understand, implement, and computationally efficient. It requires minimal tuning and can be applied quickly to large datasets.\n",
    "- Low Data Requirements: Naive Bayes can perform well even with small training datasets, making it suitable for scenarios with limited data.\n",
    "- Effective for Text Classification: It excels in text classification tasks (spam filtering, sentiment analysis) where the feature independence assumption is not severely violated.\n",
    "\n",
    "Disadvantages of the Naive Approach:\n",
    "\n",
    "- Strong Independence Assumption: The feature independence assumption is often unrealistic in real-world data, leading to suboptimal performance when features are highly correlated.\n",
    "- Limited Expressiveness: Naive Bayes may not capture complex relationships between variables, limiting its effectiveness in certain tasks.\n",
    "- Sensitivity to Irrelevant Features: It can be sensitive to irrelevant features, potentially affecting the classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6c37f",
   "metadata": {},
   "source": [
    "# Q5.\n",
    "### Can the Naive Approach be used for regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959b612",
   "metadata": {},
   "source": [
    "The Naive Approach, particularly the Naive Bayes classifier, is primarily designed for classification tasks and is not directly applicable to regression problems. \n",
    "There is an extension of Naive Bayes called the \"Gaussian Naive Bayes\" that can be adapted for simple regression problems where the target variable follows a Gaussian (normal) distribution. In Gaussian Naive Bayes regression, the algorithm assumes that the features are conditionally independent given the target variable and that each feature follows a Gaussian distribution.\n",
    "\n",
    "The steps to use Gaussian Naive Bayes for regression are as follows:\n",
    "\n",
    "1. Transform the target variable: If the target variable is not already normally distributed, apply a suitable transformation (e.g., log transformation) to make it approximately Gaussian.\n",
    "2. Model feature distributions: For each feature, estimate the mean and variance of its distribution given the target variable.\n",
    "3. Calculate the conditional probabilities: Use the Gaussian probability density function to calculate the likelihood of each feature given the target value.\n",
    "4. Make predictions: Combine the conditional probabilities using Bayes' theorem to make predictions for new data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d006bd",
   "metadata": {},
   "source": [
    "# Q6.\n",
    "### How do you handle categorical features in the Naive Approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5fc99",
   "metadata": {},
   "source": [
    "Handling categorical features in the Naive Approach, involves converting these categorical attributes into a numerical format. \n",
    "\n",
    "1. Label Encoding: In this method, each category in a categorical feature is assigned a unique integer label. For example, if you have a feature \"Color\" with categories \"Red,\" \"Blue,\" and \"Green,\" you can map them to 0, 1, and 2, respectively. However, be cautious with this approach, as it implies an ordinal relationship between the categories, which may not be the case for all categorical variables.\n",
    "2. One-Hot Encoding: This method creates binary columns for each category in the original feature. Each category becomes a new feature, and its presence is denoted by a 1, while all other columns are set to 0. This way, there is no implied ordinal relationship between the categories. For example, if \"Color\" has three categories, three binary features (e.g., \"Red,\" \"Blue,\" \"Green\") are created, each representing the presence of the corresponding color."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d08b2",
   "metadata": {},
   "source": [
    "# Q7.\n",
    "### What is Laplace smoothing and why is it used in the Naive Approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65027afb",
   "metadata": {},
   "source": [
    "Laplace smoothing, also known as add-one smoothing or additive smoothing, is a technique used to address the problem of zero probabilities in the Naive Approach. \n",
    "\n",
    "In the Naive Bayes algorithm, when calculating the probability of a feature given a class label, there is a possibility of encountering zero probabilities. This occurs when a particular feature does not appear with a specific class in the training data, resulting in a probability of zero. When zero probabilities are encountered, it can lead to severe issues during classification, such as undefined probabilities and an inability to make predictions.\n",
    "\n",
    "Laplace smoothing solves this problem by adding a small constant value (typically 1) to both the numerator and denominator of the probability calculation. \n",
    "\n",
    "The formula for Laplace smoothed probability for a feature (x) given a class (c) is:\n",
    "\n",
    "**P(x|c) = (count(x, c) + 1) / (count(c) + |V|)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2532825",
   "metadata": {},
   "source": [
    "# Q8.\n",
    "### How do you choose the appropriate probability threshold in the Naive Approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee948531",
   "metadata": {},
   "source": [
    "Choosing the appropriate probability threshold in the Naive Approach depends on the specific requirements of the application and the trade-off between precision and recall.\n",
    "\n",
    "Here's how we can choose an appropriate probability threshold:\n",
    "\n",
    "1. **Understanding the Problem**: Consider the specific problem and the consequences of false positives and false negatives. For example, in medical diagnosis, false positives may lead to unnecessary treatments, while false negatives can be life-threatening. Adjust the threshold to prioritize the more critical outcome.\n",
    "2. **Precision-Recall Trade-off**: Lowering the threshold increases recall (sensitivity) but decreases precision. Conversely, raising the threshold increases precision but decreases recall. Assess the balance needed based on the application.\n",
    "3. **Receiver Operating Characteristic (ROC) Curve**: Plot the ROC curve and analyze the trade-off between true positive rate (recall) and false positive rate. The optimal threshold is often associated with the point closest to the top-left corner (maximizing true positive rate while minimizing false positive rate).\n",
    "4. **F1 Score or Area Under the Curve (AUC)**: Consider metrics like F1 score (harmonic mean of precision and recall) or AUC to find an optimal threshold that maximizes performance.\n",
    "5. **Cross-Validation**: Use cross-validation to evaluate different thresholds and choose the one that provides the best balance of performance on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a7727a",
   "metadata": {},
   "source": [
    "# Q9.\n",
    "### Give an example scenario where the Naive Approach can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab0de7d",
   "metadata": {},
   "source": [
    "A common example scenario where the Naive Approach can be applied is in text classification, particularly for spam email filtering.\n",
    "\n",
    "**Scenario: Spam Email Filtering**\n",
    "\n",
    "In this scenario, the task is to classify incoming emails as either \"spam\" or \"non-spam\" (ham). The goal is to automatically filter out unwanted spam emails and ensure that legitimate emails reach the inbox.\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "1. **Data Collection**: Gather a labeled dataset consisting of emails, where each email is labeled as \"spam\" or \"ham.\"\n",
    "2. **Data Preprocessing**: Convert the emails into a numerical format by using techniques like bag-of-words or TF-IDF (Term Frequency-Inverse Document Frequency) to represent the text as feature vectors.\n",
    "3. **Naive Bayes Model**: Train a Naive Bayes classifier using the training set, where the features represent the words in the emails, and the labels are \"spam\" or \"ham.\"\n",
    "4. **Probability Calculation**: Calculate the probabilities of each word appearing in spam and non-spam emails, based on the training data.\n",
    "5. **Classification**: For new, unseen emails, use the Naive Bayes model to predict whether each email is \"spam\" or \"ham\" by combining the probabilities of the words present in the email.\n",
    "6. **Threshold Selection**: Apply an appropriate probability threshold to convert the continuous probabilities into discrete class labels.\n",
    "7. **Evaluation**: Measure the performance of the Naive Bayes classifier using metrics such as accuracy, precision, recall, and F1 score on a separate test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eed65d",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034b6f1",
   "metadata": {},
   "source": [
    "# Q10.\n",
    "### What is the K-Nearest Neighbors (KNN) algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef46cf",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple and popular supervised machine learning algorithm used for classification and regression tasks. It is a non-parametric, instance-based learning method, meaning it does not make any assumptions about the underlying data distribution. Instead, it relies on the proximity of data points to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b0dcd1",
   "metadata": {},
   "source": [
    "# Q 11.\n",
    "### How does the KNN algorithm work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8de0a",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm works as follows:\n",
    "\n",
    "1. Training Phase:\n",
    "- The algorithm simply stores the entire training dataset with labeled data points (instances) and their corresponding class labels (for classification tasks) or target values (for regression tasks).\n",
    "2. Prediction Phase (Classification):\n",
    "- Given a new, unlabeled data point to classify, KNN calculates the distance between this data point and all the data points in the training dataset. The most common distance metric used is the Euclidean distance, but other metrics like Manhattan distance or cosine similarity can also be used.\n",
    "- The algorithm selects the K data points (neighbors) with the smallest distances to the new data point.\n",
    "- The class label of the new data point is determined by majority voting among the K neighbors. The new data point is assigned the class label that occurs most frequently among its K nearest neighbors.\n",
    "3. Prediction Phase (Regression):\n",
    "- For regression tasks, the process is similar to the classification step, except that instead of using majority voting, KNN calculates the average or weighted average of the target values of the K nearest neighbors. This average value becomes the predicted target value for the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8958f",
   "metadata": {},
   "source": [
    "# Q 12.\n",
    "### How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b7aee3",
   "metadata": {},
   "source": [
    "Choosing the value of K in K-Nearest Neighbors (KNN) is a critical step that can significantly impact the performance of the algorithm. The appropriate value of K depends on the specific dataset and the complexity of the underlying data distribution.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95df74b",
   "metadata": {},
   "source": [
    "# Q 13.\n",
    "### What are the advantages and disadvantages of the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ca578",
   "metadata": {},
   "source": [
    "Advantages of the K-Nearest Neighbors (KNN) algorithm:\n",
    "\n",
    "1. Simple and Intuitive: KNN is easy to understand and implement, making it a great choice for beginners in machine learning.\n",
    "2. Non-Parametric: KNN makes no assumptions about the underlying data distribution, making it suitable for various types of data.\n",
    "3. Adapts to Complex Decision Boundaries: KNN can handle complex decision boundaries and nonlinear relationships between features.\n",
    "4. No Training Phase: KNN is a lazy learner, so there is no explicit training phase. It memorizes the entire training dataset, making predictions faster during the testing phase.\n",
    "5. Suitable for Multi-Class Classification: KNN can handle multi-class classification tasks without modification.\n",
    "\n",
    "Disadvantages of the KNN algorithm:\n",
    "\n",
    "1. Computationally Expensive: KNN can be computationally expensive, especially with large datasets, as it requires calculating distances between the new data point and all training data points.\n",
    "2. Sensitivity to Outliers: KNN is sensitive to outliers, as they can significantly affect the distance calculations and lead to incorrect predictions.\n",
    "3. Need for Feature Scaling: KNN performance can be affected by the scale of features, requiring proper feature scaling before training.\n",
    "4. High Memory Usage: Since KNN stores the entire training dataset during prediction, it can consume a considerable amount of memory for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf25241",
   "metadata": {},
   "source": [
    "# Q 14.\n",
    "### How does the choice of distance metric affect the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a59016",
   "metadata": {},
   "source": [
    "The choice of distance metric in K-Nearest Neighbors (KNN) can significantly impact the performance of the algorithm. The distance metric determines how the similarity or dissimilarity between data points is calculated, which, in turn, affects how KNN finds the nearest neighbors and makes predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c82188",
   "metadata": {},
   "source": [
    "# Q 15.\n",
    "### Can KNN handle imbalanced datasets? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635756e",
   "metadata": {},
   "source": [
    "Yes, K-Nearest Neighbors (KNN) can handle imbalanced datasets to some extent, but it requires certain considerations and techniques to address the challenges posed by class imbalance.\n",
    "\n",
    "Here are some ways KNN can handle imbalanced datasets:\n",
    "\n",
    "1. Selecting the Right K: The choice of K can influence how KNN handles imbalanced data. A smaller K might give more weight to the local neighborhood, which can be beneficial for handling minority class instances. However, excessively small K values can also introduce noise and lead to overfitting, so it's essential to find a balance through experimentation and cross-validation.\n",
    "2. Weighted Voting: Implement weighted voting in KNN to give more importance to the nearest neighbors when they belong to the minority class. For example, the weight can be inversely proportional to the distance from the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c0705",
   "metadata": {},
   "source": [
    "# Q 16.\n",
    "### How do you handle categorical features in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115bd145",
   "metadata": {},
   "source": [
    "Handling categorical features in K-Nearest Neighbors (KNN) requires converting these features into a numerical format since KNN relies on distance calculations in the feature space. There are two common methods to handle categorical features in KNN:\n",
    "\n",
    "1. Label Encoding: In this method, each category in a categorical feature is assigned a unique integer label. For example, if we have a feature \"Color\" with categories \"Red,\" \"Blue,\" and \"Green,\" we can map them to 0, 1, and 2, respectively. However, be cautious with this approach, as it implies an ordinal relationship between the categories, which may not be the case for all categorical variables.\n",
    "2. One-Hot Encoding: This method creates binary columns for each category in the original feature. Each category becomes a new feature, and its presence is denoted by a 1, while all other columns are set to 0. This way, there is no implied ordinal relationship between the categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9554f9",
   "metadata": {},
   "source": [
    "# Q 17.\n",
    "### What are some techniques for improving the efficiency of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d31ee",
   "metadata": {},
   "source": [
    "Improving the efficiency of K-Nearest Neighbors (KNN) is crucial, especially when dealing with large datasets or high-dimensional feature spaces. Here are some techniques to enhance the efficiency of KNN:\n",
    "\n",
    "1. Feature Scaling: Properly scale the features before applying KNN. Standardize the features to have a mean of 0 and a standard deviation of 1 (z-score normalization). Feature scaling ensures that all features contribute equally to the distance calculations.\n",
    "2. Dimensionality Reduction: Reduce the dimensionality of the feature space using techniques like Principal Component Analysis (PCA) or t-distributed Stochastic Neighbor Embedding (t-SNE). Reducing the number of features can significantly speed up KNN, especially in high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44fe6c8",
   "metadata": {},
   "source": [
    "# Q 18.\n",
    "### Give an example scenario where KNN can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aea80d",
   "metadata": {},
   "source": [
    "One example scenario where K-Nearest Neighbors (KNN) can be applied is in the field of personalized movie recommendations.\n",
    "\n",
    "**Scenario: Personalized Movie Recommendations**\n",
    "\n",
    "In this scenario, the goal is to recommend movies to users based on their preferences and historical movie ratings. The dataset contains information about users' movie ratings and the genre of each movie.\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "1. Data Collection: Gather a dataset of movie ratings from users, where each rating is associated with a specific movie and user ID. Additionally, collect information about the genre of each movie.\n",
    "2. Data Preprocessing: Encode categorical features like movie genres using one-hot encoding to convert them into a numerical format. Scale the numerical features, such as movie ratings, using feature scaling techniques.\n",
    "3. KNN Model: Train a KNN classifier or regressor using the training dataset, where the features represent movie ratings and genres, and the labels are the user IDs.\n",
    "4. Nearest Neighbors: When a new user rates a movie or expresses their preferences, use the trained KNN model to find the K nearest neighbors (users) in the feature space, based on their movie ratings and genre preferences.\n",
    "5. Recommendation: The KNN model predicts the user ID for the new user based on the majority voting of the K nearest neighbors. The recommended movies for the new user are the top-rated movies of the selected neighbors that the new user has not yet watched.\n",
    "6. Ealuation: Measure the performance of the recommendation system using evaluation metrics like precision, recall, or Mean Average Precision (MAP) on a separate test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955c236",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f55ebb",
   "metadata": {},
   "source": [
    "# Q 19.\n",
    "### What is clustering in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51188cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "175eb4e2",
   "metadata": {},
   "source": [
    "# Q 20.\n",
    "### Explain the difference between hierarchical clustering and k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408524a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30c7b6db",
   "metadata": {},
   "source": [
    "# Q 21.\n",
    "### How do you determine the optimal number of clusters in k-means clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e34446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "243fe03c",
   "metadata": {},
   "source": [
    "# Q 22.\n",
    "### What are some common distance metrics used in clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b5c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ce79fa0",
   "metadata": {},
   "source": [
    "# Q 23.\n",
    "### How do you handle categorical features in clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb4df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d033c65",
   "metadata": {},
   "source": [
    "# Q 24.\n",
    "### What are the advantages and disadvantages of hierarchical clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b8a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57e73eb5",
   "metadata": {},
   "source": [
    "# Q 25.\n",
    "### Explain the concept of silhouette score and its interpretation in clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e316d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07df4fd6",
   "metadata": {},
   "source": [
    "# Q 26.\n",
    "### Give an example scenario where clustering can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922bf99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "966b8393",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d533d",
   "metadata": {},
   "source": [
    "# Q 27.\n",
    "### What is anomaly detection in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d49c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f73ffa71",
   "metadata": {},
   "source": [
    "# Q 28.\n",
    "### Explain the difference between supervised and unsupervised anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3b0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f638ccb",
   "metadata": {},
   "source": [
    "# Q 29.\n",
    "### What are some common techniques used for anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552ccd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b59494c7",
   "metadata": {},
   "source": [
    "# Q 30.\n",
    "### How does the One-Class SVM algorithm work for anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a7cce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b7d700d",
   "metadata": {},
   "source": [
    "# Q 31.\n",
    "### How do you choose the appropriate threshold for anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6cb814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0350814c",
   "metadata": {},
   "source": [
    "# Q 32.\n",
    "### How do you handle imbalanced datasets in anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac3b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1a5a03b",
   "metadata": {},
   "source": [
    "# Q 33.\n",
    "### Give an example scenario where anomaly detection can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9146333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c8aa5e4",
   "metadata": {},
   "source": [
    "## Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd5868e",
   "metadata": {},
   "source": [
    "# Q 34.\n",
    "###  What is dimension reduction in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee09af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c3ffbd6",
   "metadata": {},
   "source": [
    "# Q 35.\n",
    "### Explain the difference between feature selection and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4a80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2601636",
   "metadata": {},
   "source": [
    "# Q 36.\n",
    "### How does Principal Component Analysis (PCA) work for dimension reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b908158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dae7cc4",
   "metadata": {},
   "source": [
    "# Q 37.\n",
    "### How do you choose the number of components in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259e1050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4603169",
   "metadata": {},
   "source": [
    "# Q 38.\n",
    "### What are some other dimension reduction techniques besides PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215afda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c0aa6fe",
   "metadata": {},
   "source": [
    "# Q 39.\n",
    "### Give an example scenario where dimension reduction can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e791ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "383e687a",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f6df0",
   "metadata": {},
   "source": [
    "# Q 40.\n",
    "### What is feature selection in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e62c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e776ebc",
   "metadata": {},
   "source": [
    "# Q 41.\n",
    "### Explain the difference between filter, wrapper, and embedded methods of feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d176a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2d33802",
   "metadata": {},
   "source": [
    "# Q 42.\n",
    "### How does correlation-based feature selection work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4ad25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f773eaa6",
   "metadata": {},
   "source": [
    "# Q 43.\n",
    "### How do you handle multicollinearity in feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf2b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71fa0a77",
   "metadata": {},
   "source": [
    "# Q 44.\n",
    "### What are some common feature selection metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c9f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c80369b9",
   "metadata": {},
   "source": [
    "# Q 45.\n",
    "### Give an example scenario where feature selection can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505d3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3ef36da",
   "metadata": {},
   "source": [
    "## Data Drift Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83394f7f",
   "metadata": {},
   "source": [
    "# Q 46.\n",
    "### What is data drift in machine learning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeaecce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8051ce3b",
   "metadata": {},
   "source": [
    "# Q 47.\n",
    "### Why is data drift detection important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96129315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "223768a3",
   "metadata": {},
   "source": [
    "# Q 48.\n",
    "### Explain the difference between concept drift and feature drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c5b34d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0827927",
   "metadata": {},
   "source": [
    "# Q 49.\n",
    "### What are some techniques used for detecting data drift?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506bd35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a3f81d0",
   "metadata": {},
   "source": [
    "# Q 50.\n",
    "### How can you handle data drift in a machine learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18a215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c96e00b9",
   "metadata": {},
   "source": [
    "## Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c92781",
   "metadata": {},
   "source": [
    "# Q 51.\n",
    "### What is data leakage in machine learning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf5065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a79163f4",
   "metadata": {},
   "source": [
    "# Q 52.\n",
    "### Why is data leakage a concern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fa1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de892cab",
   "metadata": {},
   "source": [
    "# Q 53.\n",
    "### Explain the difference between target leakage and train-test contamination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb70463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a832af8f",
   "metadata": {},
   "source": [
    "# Q 54.\n",
    "### How can you identify and prevent data leakage in a machine learning pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf503c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaacfd8b",
   "metadata": {},
   "source": [
    "# Q 55.\n",
    "### What are some common sources of data leakage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb18fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e027459",
   "metadata": {},
   "source": [
    "# Q 56.\n",
    "### Give an example scenario where data leakage can occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded329c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ef92b32",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089827be",
   "metadata": {},
   "source": [
    "# Q 57.\n",
    "### What is cross-validation in machine learning?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f309a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb680898",
   "metadata": {},
   "source": [
    "# Q 58.\n",
    "### Why is cross-validation important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d39ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3751d8e",
   "metadata": {},
   "source": [
    "# Q 59.\n",
    "### Explain the difference between k-fold cross-validation and stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6104be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dd9591a",
   "metadata": {},
   "source": [
    "# Q 60.\n",
    "### How do you interpret the cross-validation results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27393c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
